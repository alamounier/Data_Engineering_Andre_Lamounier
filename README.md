# ğŸ—ï¸ Projeto de ETL com Airflow, Docker, Delta Lake e Arquitetura MedalhÃ£o

Este projeto implementa um pipeline de ETL utilizando Apache Airflow, PySpark, Docker e Delta Lake, estruturado em uma arquitetura de dados em camadas (Bronze, Silver e Gold). O objetivo Ã© orquestrar o processamento de dados da API Open Brewery em contÃªineres separados para cada etapa do pipeline.

---

## ğŸ”§ Tecnologias Utilizadas

- **Apache Airflow** (orquestraÃ§Ã£o de workflows)
- **Apache Spark com Delta Lake** (processamento de dados com transaÃ§Ãµes ACID)
- **Docker** (ambiente isolado para cada etapa de processamento)
- **Docker Compose** (gerenciamento de mÃºltiplos serviÃ§os)
- **Python 3**

---

## ğŸ§± Arquitetura MedalhÃ£o

A arquitetura Ã© dividida em trÃªs camadas:

- **Bronze**: coleta dados crus da API Open Brewery.
- **Silver**: limpa, transforma e particiona os dados por localizaÃ§Ã£o.
- **Gold**: agrega dados para anÃ¡lises, como a contagem de cervejarias por tipo e localidade.

---

## ğŸ“ Estrutura do Projeto

<img src="/imgs_png/estrutura_projeto.png" alt="python" height="50" /> 

## ğŸ“ Como executar

1. Suba os serviÃ§os com Docker Compose
Execute o seguinte comando na raiz do projeto (onde estÃ¡ localizado o docker-compose.yml):

- docker-compose up -d

2. Acesse a interface do Airflow
Abra seu navegador e vÃ¡ para: http://localhost:8080
Use as credenciais padrÃ£o:

- UsuÃ¡rio: airflow
- Senha: airflow

3. Ative e execute a DAG
Na interface do Airflow:

- Localize a DAG chamada brewery_etl_dag.
- Habilite a DAG clicando no botÃ£o "On/Off".
- Em seguida, clique em Trigger DAG (botÃ£o de play) para iniciar o pipeline.

4. Monitore as execuÃ§Ãµes
Acompanhe o progresso e visualize os logs diretamente na interface do Airflow, clicando em cada task da DAG.
Os resultados processados serÃ£o salvos na pasta src/outputs.
