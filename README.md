# üèóÔ∏è Projeto de ETL com Airflow, Docker, Delta Lake e Arquitetura Medalh√£o

Este projeto implementa um pipeline de ETL utilizando Apache Airflow, PySpark, Docker e Delta Lake, estruturado em uma arquitetura de dados em camadas (Bronze, Silver e Gold). O objetivo √© orquestrar o processamento de dados da API Open Brewery em cont√™ineres separados para cada etapa do pipeline.

<div align="center">
  <img src="/imgs_png/arquitetura_projeto.png" alt="python" height="200">
</div>

---

## üîß Pr√© Requisitos

- **Docker instalado na m√°quina local** 

---

## üîß Tecnologias Utilizadas

- **Apache Airflow** (orquestra√ß√£o de workflows)
- **Apache Spark com Delta Lake** (processamento de dados com transa√ß√µes ACID)
- **Docker** (ambiente isolado para cada etapa de processamento)
- **Docker Compose** (gerenciamento de m√∫ltiplos servi√ßos)
- **Python 3**

---

## üß± Arquitetura Medalh√£o

A arquitetura √© dividida em tr√™s camadas:

- **Bronze**: coleta dados crus da API Open Brewery.
- **Silver**: limpa, transforma e particiona os dados por localiza√ß√£o.
- **Gold**: agrega dados para an√°lises, como a contagem de cervejarias por tipo e localidade.

---

## üìÅ Estrutura do Projeto

<img src="/imgs_png/estrutura_projeto.png" alt="python" height="300" /> 

## (colocar engrenagem) Como executar

Siga os passos abaixo para rodar este projeto:

1. Copie o diret√≥rio do projeto para uma pasta local em seu computador.

2. Abra o terminal do seu computador e mova at√© o diret√≥rio do projeto.

3. Entra na pasta src/jars e baixa o conector do pypsark no seguinte link https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.1026/aws-java-sdk-bundle-1.11.1026.jar e salve nesta mesma pasta, retorne a pasta do projeto raiz novamente pelo terminal.

4. Crie a imagem do container do PySpark executando o seguinte comando: `docker build -t pyspark_image .`

5. Navegue at√© a pasta do Airflow no terminal, aguarde a execu√ß√£o do container do PySpark e, em seguida, crie o container do Airflow com o seguinte comando: `docker-compose up -d`


## üß† L√≥gica

O objetivo deste pipeline √© aproveitar as vantagens da engine Delta e da arquitetura Medalh√£o para garantir um processamento eficiente, mantendo o controle sobre o hist√≥rico de dados e a vers√£o mais recente para an√°lises.

Camada Bronze: Armazena os dados brutos provenientes das fontes, mantendo todas as vers√µes dos registros. Utilizando o Change Data Feed (CDF), √© poss√≠vel rastrear qualquer altera√ß√£o nos dados ao longo do tempo. Isso oferece um hist√≥rico completo de todas as mudan√ßas feitas nos dados de entrada.

Camada Silver: A partir dessa camada, o pipeline processa e transforma os dados para um formato mais adequado para an√°lise. A tabela da camada Silver ser√° sobrescrita a cada execu√ß√£o, garantindo que apenas a vers√£o mais recente dos dados seja mantida, o que economiza recursos de processamento. N√£o h√° necessidade de armazenar o hist√≥rico completo das altera√ß√µes, j√° que a camada Bronze j√° preserva essa informa√ß√£o.

Camada Gold: A camada Gold ser√° a tabela da camada silver em formato agregado, ideal para constru√ß√£o de dashboards e relat√≥rios para √°rea de neg√≥cios.

Vantagem do Versionamento: Caso seja necess√°rio recuperar vers√µes anteriores dos dados ou realizar auditoria, a camada Bronze com versionamento via Delta oferece essa flexibilidade. A camada Silver foca apenas na vers√£o mais atual, o que facilita a an√°lise e melhora a performance.

Esse modelo de arquitetura permite um equil√≠brio entre o controle total do hist√≥rico (na camada Bronze) e a efici√™ncia de processamento (na camada Silver), otimizando recursos e mantendo a flexibilidade para futuros ajustes ou auditorias.


